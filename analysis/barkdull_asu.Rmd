---
title: "FormicidaeMolecularEvolution - Comparative Genomic Analysis Workflow"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
    toc_float: true
    toc_collapsed: false
    theme: default
    css: "theme3.css"
toc_depth: 1
number_sections: true
editor_options:
  chunk_output_type: console
---

## Scripts and how-tos for comparative genomics work. {.tabset}
### What is FormicidaeMolecularEvolution?
This workflow was developed by Megan Barkdull of the Moreau lab at Cornell University to find orthogroups between a reference and at least 1 query species. In particular, this pipeline allows for the user to select for the longest [isoform](https://elifesciences.org/articles/34477). This feature allows us to skip analyzing repeating sequences in the *schistocerca* genomes.

On her GitHub, Barkdull provides a flowchart that summarizes the process of this pipeline and the purpose of each step. Pictured below is the simplified version of this flowchart.

<center><img src="assets/barkdull-flowchart-simple.png"></center>
<center><i><font size="-1.5">Simplified FormicidaeMolecularEvolution Pipeline Flowchart</font></i></center>

### Installation and Running 

<mark> Note: This procedure has been updated to run on the TAMU Grace cluster following the March 2025 update to the Grace OS.</mark>

#### Clone the GitHub repository
For both local and cluster installation, you just need to clone Barkdull's GitHub repository into your own environment using the command 

```{r clone-barkdull, eval=FALSE}
git clone git@github.com:mbarkdull/FormicidaeMolecularEvolution.git
```

#### Preparing your data files

Barkdull provides a script that downloads all the data you need from an FTP address, but the formatting must be within a .csv file, with the URLs formatted within the columns as follows

```{r NCBI-stats csv-format, echo=FALSE, message=FALSE, warning =FALSE}
library(knitr)
library(kableExtra)

# Data for the table
data <- data.frame(
  Column_1 = c("Coding Sequence URL"),
  Column_2 = c("Protein Sequence URL"),
  Column_3 = c("Species abbreviation code"),
  Column_4 = c("Longest isoform? (yes/no)"),
  Column_5 = c("Comments")
)

# Create the table
kable(data, escape = FALSE, caption = "") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                full_width = F) %>%
  column_spec(1, width = "30em") %>%
  column_spec(2, width = "30em") %>%
  column_spec(3, width = "30em") %>%
  column_spec(4, width = "30em") %>%
  column_spec(5, width = "30em") %>%
  row_spec(0, bold = TRUE, background = "#bfa2b0")
```

This should be done for each species, where the data for each species is on its own row. So the format would be:

    Species 1: Coding seq. URL, Protein seq. URL, ...  
    Species 2: Coding seq. URL, Protein seq. URL, ...  
    Species 3: Coding seq. URL, Protein seq. URL, ...  
  
and so on.

These files are easily created in Microsoft Excel, Google Sheets, or any spreadsheet software by entering your data in the columns as shown above and selecting the .csv option under "Save as". 

Then, move this file into the scripts folder within the cloned GitHub repository. Finally, from the FormicidaeMolecularEvolution directory run the command

```{r download-data, eval=FALSE}
./scripts/DataDownload scripts/YourInputFile.csv
```

This will download all the data you need to run this workflow.

#### Running the Pipeline 


##### Step 1: Set Up the Environment

First we need to download PanDoc in order to update our packages as needed without interference. We do this with the command

```{r configR, eval=FALSE}
module purge
module load GCC/13.2.0  OpenMPI/4.1.6 r-4.3.2-gcc

export R_LIBS=$SCRATCH/R_LIBS_USER/4.3.2-foss-2023b:$R_LIBS
```


Then, we need to load the the necessary modules onto your terminal. The commands are as follows:

```{r ml-R, eval=FALSE}
rm -rf $HOME/.R*
rm -rf $SCRATCH/.R*
rm -rf /path/to/your/FormicidaeMolecularEvolution/.R*
  
R --vanilla

library("orthologr")
library("biomartr")

quit()
```

**Is orthologr not available?** Copy and paste the code from [this link](https://github.com/drostlab/orthologr) under *Install orthologr*.

##### Step 2: Select the Longest Isoforms

The next step is the command

```{r gene-retrieval, eval=FALSE}
./scripts/GeneRetrieval.R scripts/YourInputFile.csv
```

Running this script selects for the single longest isoform in each gene then matches protein names to gene names in order to subset the coding sequence file to just the longest isoforms. This produces 2 files for each species, one containing the longest protein isoforms and another with the longest coding sequence isoforms in a directory called *2_LongestIsoforms*.

##### Step 3: Clean the Raw Data

The next step is to clean the data using the command

```{r clean-data-script-command, eval=FALSE}

./scripts/DataCleaning scripts/YourInputFile.csv

```

This script removes any special characters from gene names and adds the 4-letter taxon abbreviation to the beginning of each gene name. This creates a new directory called *3_CleanedData*.

##### Step 4: Translate Nucleotide Sequences to Amino Acid Sequences

For this next step we will need to alter one of Barkdull's files with some of our own lines.

  i) Create a file in *scripts* called *DataTranslating_modif*. 
    
  ii) Copy and paste the code from the *scripts/DataTranslating* file.
    
  iii) Replace lines 25-28 in *DataTranslating_modif* with the following code  
  ```{r add-datatranslating-lines, eval=FALSE}
       # Now we can run Transdecoder on the cleaned file:
    echo "First, attempting TransDecoder run on $cleanName"
ml GCC/12.3.0 OpenMPI/4.1.5 TransDecoder/5.7.1 Biopython/1.83
    TransDecoder.LongOrfs -t $cleanName
    TransDecoder.Predict -t $cleanName --single_best_only
  ```
      
      
      
  iv) Replace the header in *DataTranslating_modif* with the following lines
  ```{r replace-header, eval=FALSE}
    
    #!/bin/bash

    ##NECESSARY JOB SPECIFICATIONS
    #SBATCH --job-name=Transdecoder         #Set the job name to "JobExample4"
    #SBATCH --time=02:00:00         #Set the wall clock limit to 1hr and 30min
    #SBATCH --ntasks=2              #Request 2 task
    #SBATCH --cpus-per-task=4       #Request 8 task
    #SBATCH --mem=20G              #Request 50GB per node

ml GCC/12.3.0 OpenMPI/4.1.5 TransDecoder/5.7.1 Biopython/1.83
    
  ```

  v) Run the script using the command
  ```{r run-datatrans-modif, eval=FALSE}
  sbatch ./scripts/DataTranslating_modif ./scripts/YourInputFile.csv
  ```
  
##### Step 5: Running OrthoFinder
For this step we will not be using anything from the original pipeline's steps, instead we will create our own directory and copy over the translated fasta files as follows:

```{r step-5-altered, eval=FALSE}
  mkdir LocustsGenomeEvolution
  mkdir LocustsGenomeEvolution/tmp
  mkdir 5_OrthoFinder
  mkdir 5_OrthoFinder/fasta
  cp ./4_1_TranslatedData/OutputFiles/translated* ./5_OrthoFinder/fasta
  cd ./5_OrthoFinder/fasta
  rename translated '' translated*
  cd ..
  cd ..
```

Then to run OrthoFinder on the Grace cluster we will create a file within the *scripts* directory called *run-ortho.sh*. Copy and paste the following code into *run-ortho.sh*
```{r run-ortho-grace, eval=FALSE}

#!/bin/bash

##NECESSARY JOB SPECIFICATIONS
#SBATCH --job-name=orthofinder         #Set the job name to "orthofinder"
#SBATCH --time=06:00:00         #Set the wall clock limit to 6 hours
#SBATCH --ntasks=2              #Request 2 task
#SBATCH --cpus-per-task=4       #Request 4 task
#SBATCH --mem=30G              #Request 30GB per node

ml purge
ml GCC/12.3.0 OpenMPI/4.1.5 OrthoFinder/2.5.5 IQ-TREE/2.3.6 fasttree/2.1.11 MAFFT/7.520-with-extensions

orthofinder -S diamond -T iqtree -A mafft -I 5 -t 32 -a 4 -M msa -f ${file path to 5_OrthoFinder/fasta} -p ${file path to LocustsGenomeEvolution/tmp}

```

<details>
  <summary>What do these flags mean? (-S, -T, etc.)</summary>
  
  -S diamond: Use diamond as the sequence search program.
  
  -T iqtree: Use iqtree as the tree inference program.
  
  -A mafft: Use mafft as the multiple sequence alignment (MSA) program.
  
  -I 5: Sets the MCL inflation parameter to 5.
  
  -t 32: Sets the number of threads to 32.
  
</details>

and then we can run our slurm script using 

```{r run-ortho-submit, eval=FALSE}

sbatch scripts/run-ortho.sh

```


##### Step 6: Orthogroups to GeneID

First we must remove the suffixes that the pipeline puts in front of our protein coding sequences, so create a file called *remove-suffix.py* in the directory 5_OrthoFinder/fasta/OrthoFinder/Results_{DATE-OF-RUN}/Orthogroups and copy the following code into it

```{r remove-suff.py, eval=FALSE}

import re

# Define the input and output file names
input_file = 'Orthogroups.txt'
output_file = 'Orthogroups_reprocessed.txt'

# Read the input file
with open(input_file, 'r') as file:
    data = file.read()

# Remove all prefixes before any occurrence of "_XP", "_NP", and "_YP"
result = re.sub(r'\b\w+_(XP|NP|YP)', r'\1', data)

# Write the result to the output file
with open(output_file, 'w') as file:
    file.write(result)

print(f"Processed data has been written to {output_file}")

```


##### Step 7: Create a Table and Clean the Data
We then need to create a table with each protein and gene ID for all species present, so we must use the the script *protein2geneid_loop.py* by David Emms, a software developer for OrthoFinder.

Create a file called *protein2geneid_loop.py* in your *1_RawData* directory and copy and paste the following code into it

```{r protein2geneid.py, eval=FALSE}

import os
import re

# Get the current working directory
gff_directory = os.getcwd()

# Create a directory to store the output files
output_directory = os.path.join(gff_directory, "output_files")
os.makedirs(output_directory, exist_ok=True)

# List GFF files with "_GFF.gff" extension
species_list = [filename for filename in os.listdir(gff_directory) if filename.endswith("_GFF.gff")]

# Process each species' GFF file
for species_filename in species_list:
    # Extract the species name from the file name
    species_name = re.sub(r"_GFF\.gff$", "", species_filename)
    
    # Construct input and output file paths
    input_path = os.path.join(gff_directory, species_filename)
    output_filename = f"xp{species_name}.gff"  # Remove the dot before species_name
    output_path = os.path.join(output_directory, output_filename)
    
    # Use 'grep' to filter lines containing "XP" and save to the output file
    grep_command = f'grep "XP" "{input_path}" > "{output_path}"'
    os.system(grep_command)
    
    # Print a message indicating the filtering process
    print(f"Filtered {species_filename} to {output_filename}")

    # Construct the output TSV file path
    tsv_output_filename = f'gffKey{species_name}.tsv'  # Remove the dot before species_name
    tsv_output_path = os.path.join(output_directory, tsv_output_filename)
    lol = {}

    # Read and process the contents of the GFF file
    with open(output_path) as gffFile:
        for line in gffFile:
            if re.search(r"product=[^;]+", line):
                gene_match = re.search(r"gene=[^;=]+", line)
                product_match = re.search(r"product=[^;]+", line)
                proteinID_match = re.search(r"protein_id=(.+)", line)
                
                if gene_match and product_match and proteinID_match:
                    gene = gene_match.group(0)
                    if gene.endswith('product'):
                        gene = gene[5:-7]
                    else:
                        gene = gene[5:]
                    
                    product = product_match.group(0)
                    product = product.replace("product=", "")
                    product = product.rstrip()
                    
                    proteinID = proteinID_match.group(1)
                    
                    if proteinID not in lol.keys():
                        lol[proteinID] = [gene, product, species_name]  # Add species name

    # Write the processed data to the output TSV file
    with open(tsv_output_path, 'w') as output:
        for proID, lis in lol.items():
            output.writelines(proID + '\t' + lis[0] + '\t' + lis[1] + '\t' + lis[2] + '\n')  # Include species name

    # Print a message indicating the processing of the GFF file
    print(f"Processed {output_filename} to {tsv_output_filename}")

# Concatenate all final files into a single file
final_output_filename = "allspecies_protein2geneid.tsv"
final_output_path = os.path.join(output_directory, final_output_filename)

with open(final_output_path, 'w') as final_output:
    for species_filename in species_list:
        species_name = re.sub(r"_GFF\.gff$", "", species_filename)
        tsv_output_filename = f'gffKey{species_name}.tsv'  # Remove the dot before species_name
        tsv_output_path = os.path.join(output_directory, tsv_output_filename)
        
        with open(tsv_output_path, 'r') as species_output:
            final_output.write(species_output.read())

print(f"Concatenated all species files into {final_output_filename}")

```

Then run this script in the folder *1_RawData* using

```{r run-protein2geneid.py, eval=FALSE}

cd {PATH TO 1_RawData}

python3 protein2geneid_loop.py

```

##### Step 8: Correct Possible Errors in Table
After we get this table, we need to run this script to clean some small errors that may occur in the table. Create a file named *clean-table.R* in your main OrthoFinder directory and copy and paste the following code into it

```{r clean-table.R, eval=FALSE}
library(readr)
library(tidyverse)

orthogroup <- read_table("data/orthologs/Orthogroups_reprocessed.txt", col_names = FALSE)

# We reshape the table so that we have one orthogroup line associated with one protein id
transformed_orthogroup <- orthogroup %>%
  pivot_longer(cols = -X1, names_to = "Protein_Column", values_to = "protein_id") %>%
  replace_na(list(X1 = "N/A")) %>%
  select(orthogroup = X1, protein_id) %>%
  filter(!is.na(protein_id))

# Export the table to  tab-separated text file (you can change the delimiter if needed)
write.table(transformed_orthogroup, file = "data/orthologs/Orthogroups_11species_May2024.txt", sep = "\t", quote = FALSE, row.names = FALSE)

proteingeneid <- read_tsv("data/orthologs/allspecies_protein2geneid.tsv", col_names = TRUE ) 

final_orthotable <- left_join(transformed_orthogroup, proteingeneid, by = "protein_id")
write.table(final_orthotable, file = "data/orthologs/Orthogroups_genesprotein_11species_May2024.csv", sep = ",", quote = FALSE, row.names = FALSE)

```

We can run this script in the main directory using the following commands:

```{r runR, eval=FALSE}
module purge
module load GCC/13.2.0  OpenMPI/4.1.6 R_tamu/4.3.2

Rscript clean-table.R
```

Finally, we have finished and gotten a table of all species and their corresponding IDs.

### Troubleshooting

##### No Module named Bio
This error occurs when the program can't find BioPython. Before the "ml" line in your script add the command 

```{r purge, eval=FALSE}
module purge
```

This will get rid of all previously loaded packages before you reload the ones you need for this script. Not quite sure why this works but it does. 

##### Species on Last Line of .csv File Not Being Processed
There is a glitch in this pipeline that causes the program to index 1 row short of the actual number of rows. To correct this, I just copy and paste the last line of my data entries on the line right below the "real" entries. This dummy line will correct the indexing issues and in the event it is read, the line will not cause any issues with the data processing. Pictured below is a screenshot of my .csv file with my dummy line highlighted in green. Notice it is a duplicate of the line right above it.

<center><img src="assets/dummy-line.png"></center>
<center><i><font size="-1.5">Example of .csv with dummy line</font></i></center>

##### Species Tree Inference Failed
David Emms says that sometimes when running OrthoFinder with IQ-Tree, it will randomly throw an error when in the tree inference step. The way to correct this is to manually root the tree using the following script (adapted to run on Grace from David Emms' comment [here](https://github.com/davidemms/OrthoFinder/issues/624#issuecomment-929088084) with the help of Thang Ha from the HPRC help desk):


```{r reroot-tree, eval=FALSE}

#!/bin/bash

##NECESSARY JOB SPECIFICATIONS
#SBATCH --job-name=manual-root-tree-orthofinder         #Set the job name to "manual-root-tree-orthofinder"
#SBATCH --time=7-00:00:00         #Set the wall clock limit to 7 days
#SBATCH --ntasks=2              #Request 2 task
#SBATCH --cpus-per-task=4       #Request 4 task
#SBATCH --mem=30G              #Request 30GB per node

ml purge
ml GCC/12.3.0 OpenMPI/4.1.5 OrthoFinder/2.5.5 IQ-TREE/2.3.6 FastTree/2.1.11 MAFFT/7.520-with-extensions

iqtree2 -s /Path/to/5_OrthoFinder/fasta/OrthoFinder/Results_{DATE}/WorkingDirectory/Alignments_ids/SpeciesTreeAlignment.fa -bb 1000 -pre /Path/to/5_OrthoFinder/fasta/OrthoFinder/Results_{DATE}/WorkingDirectory/Alignments_ids/SpeciesTree -safe -nt AUTO

python convert_orthofinder_tree_ids /Path/to/5_OrthoFinder/fasta/OrthoFinder/Results_{DATE}/WorkingDirectory/Alignments_ids/SpeciesTree.treefile /Path/to/5_OrthoFinder/fasta/OrthoFinder/Results_{DATE}/WorkingDirectory/SpeciesIDs.txt

python orthofinder -ft //Path/to/5_OrthoFinder/fasta/OrthoFinder/Results_{DATE} -s SpeciesTreeRooted.txt

```

This will pick up from where your run quit, root the tree, and then run OrthoFinder with the tree generated. <mark> Note: This will take longer than a regular OrthoFinder run</mark>

### Citations
Megan Barkdull. (2021). mbarkdull/FormicidaeMolecularEvolution: v1.0.0-beta (v1.0.0-beta). Zenodo. [https://doi.org/10.5281/zenodo.5668329](https://doi.org/10.5281/zenodo.5668329)
